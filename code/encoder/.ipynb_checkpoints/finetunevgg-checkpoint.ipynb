{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder - finetuned version\n",
    "\n",
    "In this notebook, I retrained the VGG16 convolution net on the Flickr dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting notebook to google drive\n",
    "First, we connect this notebook to google drive. This was done to make it easier to fetch and write files. If files were in local, then you had to upload them every time a new session started. \n",
    "\n",
    "Pydrive library handles the authentication and connection to your google drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftyyfcnFS4_r"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Rw9r59SdQxC0",
    "outputId": "1040dc48-3943-4a9a-eaab-1824c33aa5d0"
   },
   "outputs": [],
   "source": [
    "# Mount the drive on to google colab system\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lvxKEnHiS7Tj",
    "outputId": "ce6ba947-eb4a-4cb7-cba9-5704e47e8c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/fine_tune_myself\n"
     ]
    }
   ],
   "source": [
    "#Go to the directory containing the dataset.\n",
    "%cd \"/content/drive/My Drive/fine_tune_myself\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "In this section, we go through the functions used for preparing the data and model.\n",
    "\n",
    "The Flickr8K Dataset comes with files with image identifiers for training and testing images. Each identifier is in its own line. We can use it to automatically fetch files. We use it to create dataset referring to the actual image data.\n",
    "\n",
    "Also, as explained in the documentation, finetuning consists of the following process:\n",
    "\n",
    "1. Modify the VGG16 architecture with your own classification layer and initialize its weights by one epoch of training. In our case, I used the reduced vocabulary as a set of labels. The reduced vocabulary is consisting of 300 words. In the get_targets.py file, we use the captions to marks the words occuring as labels. All the words for an image coming in all 5 captions that occur in the reduced vocabulary are marked 1. The rest are 0.\n",
    "\n",
    "2. We train and retrain the modified VGG16's last convolution layer. We take the history of the model to feed it as an initial point to the next level of training. The history object keeps track of the metrics recorded in a training cycle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "oOYCLzDfS_wc",
    "outputId": "2031404a-eed9-46c8-fcc2-73995003c8eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Feb 23 21:38:45 2019\n",
    "\n",
    "@author: ghrit\n",
    "\"\"\"\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.models import Sequential, Model\n",
    "from pickle import load\n",
    "from keras.layers import Input\n",
    "\n",
    "\n",
    "# This function is simply used to read a file and return the content.\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename) #open file with identifiers.\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]  #remove the 'jpg' extension\n",
    "        dataset.append(identifier)\n",
    "    return list(set(dataset))\n",
    "\n",
    "\n",
    "# The targets were created by me for use as classification labels\n",
    "# To know more see get_targets.py\n",
    "def load_targets(fn,dset):\n",
    "    #unpickle the python dict containing the id:target mapping\n",
    "    targets = load(open(fn, 'rb'))\n",
    "    # dset is the collection of image identifiers.\n",
    "    targets = {k: targets[k] for k in dset} # only add targets of train or test dataset.\n",
    "    # returns a dict with mapping of image id to target vector\n",
    "    return targets \n",
    "\n",
    "# generator function is used to feed the VGG16 an example. It is called when compiling the predefined VGG16 keras model.\n",
    "def tr_genr(directory, targets,dset):\n",
    "    # Run loop indefinitely\n",
    "    while 1:\n",
    "        # loop through all the images in the training or testing dataset\n",
    "        for name in dset:\n",
    "            # Add the extension to complete the file name\n",
    "            filename = directory + '/' + name + \".jpg\"\n",
    "            # Use keras load_image function to get image in PIL format\n",
    "            image = load_img(filename, target_size=(224, 224))\n",
    "            # convert from PIL to a numpy array of pixels\n",
    "            image = img_to_array(image)\n",
    "            # Reshape numpy array in the format required by VGG16\n",
    "            image= image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image= preprocess_input(image)\n",
    "            # get the target vector for the image and reshape as required by keras layer\n",
    "            y = np.array(targets[name]).reshape(1,300)\n",
    "            x = np.array(image)\n",
    "            \n",
    "            # Generate an example (pair of features,labels) for the model to train on.\n",
    "            yield [x,y]\n",
    "\n",
    "# This function is used to initialize a modified VGG16 model for finetuning.\n",
    "# We train it for one epoch to initialize the parameters of the customized layer.\n",
    "# This is not the final training and is run only once.\n",
    "def get_initialized_vgg(gen, old_model):\n",
    "    \n",
    "    # remove the final layer which is based on the imagenet classification labels\n",
    "    old_model.layers.pop()\n",
    "    \n",
    "    # Take the output of the remaining model\n",
    "    x= old_model.layers[-1].output\n",
    "    \n",
    "    # Replace with layer based on classification labels for our dataset captions (length 300)\n",
    "    final = Dense(300, activation='sigmoid')(x)\n",
    "    \n",
    "    #Define model inputs and outputs\n",
    "    model = Model(inputs= old_model.input, outputs= final)\n",
    "    \n",
    "    # save the updated model architecture\n",
    "    plot_model(model, to_file='vgg2model.png',show_shapes=True)\n",
    "    \n",
    "    # We only train the final layer we added so as to use it as a representation of images\n",
    "    # freeze pre-trained model area's layer. We do not want to mess with the actual model. \n",
    "    for layer in old_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define model loss and optimizer\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "    # Train the model using the generator defined before. Steps per epoch are equal to number of example images\n",
    "    model.fit_generator(gen,steps_per_epoch=6000)\n",
    "    return model\n",
    "\n",
    "# This function trains and evaluates the CNN model.\n",
    "def train_vgg(gen,te_gen,model):\n",
    "    # set the first 25 layers (up to the last conv block)\n",
    "    # to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers[:21]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[21:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # we use a Stochastic gradient descent as optimizer.\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9)\n",
    "    \n",
    "    # we use categorical crossentropy because we have multiple labels for each image.\n",
    "    model.compile(optimizer=sgd, loss='binary_crossentropy',  \n",
    "                  metrics=['binary_accuracy','categorical_accuracy'])\n",
    "    \n",
    "    # We keep track of the metrics using the history callback provided by keras.\n",
    "    history = model.fit_generator(gen,steps_per_epoch=6000, epochs=5,\n",
    "                                  validation_data=te_gen,validation_steps=1000)\n",
    "    return model  ,history  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together\n",
    "\n",
    "The images are stored in my drive so we specify that directory. We also specify the shape of the input layer required by VGG16 model. Then, we initialize the existing VGG16 model provided by keras with imagenet trained weights. We can also use an untrained VGG model. However, the point of finetuning is to take advantage of the existing model by extending it with your dataset. Retraining VGG would require a huge dataset for good results and we do not have required resources.\n",
    "\n",
    "\n",
    "We also get the training and testing generators ready with the appropriate images and targets. This is commented because we only need to execute this code once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "ZNqoSL0pS_0B",
    "outputId": "3e6bd350-4148-40f7-d848-2f8f1a2dc8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 6s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrain = load_set(\\'Flickr_8k.trainImages.txt\\')\\ntr_targets=load_targets(\"targets300.pkl\",train[:])\\ntr_gen=tr_genr(directory,tr_targets,train[:])\\n\\ntest =load_set(\\'Flickr_8k.testImages.txt\\')\\nte_targets=load_targets(\"targets300.pkl\",test)\\nte_gen=tr_genr(\\'/content/drive/My Drive/Multi-label-Inception-net/testim\\',te_targets,test)'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory='/content/drive/My Drive/Multi-label-Inception-net/image/images'\n",
    "input_tensor = Input(shape=(224,224,3))\n",
    "vggmodel = VGG16(weights='imagenet', include_top=True,input_tensor=input_tensor)\n",
    "plot_model(vggmodel,to_file='vgg_orig.png',show_shapes=True)\n",
    "'''\n",
    "train = load_set('Flickr_8k.trainImages.txt')\n",
    "tr_targets=load_targets(\"targets300.pkl\",train[:])\n",
    "tr_gen=tr_genr(directory,tr_targets,train[:])\n",
    "\n",
    "test =load_set('Flickr_8k.testImages.txt')\n",
    "te_targets=load_targets(\"targets300.pkl\",test)\n",
    "te_gen=tr_genr('/content/drive/My Drive/Multi-label-Inception-net/testim',te_targets,test)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing VGG16\n",
    "\n",
    "We create a VGG16 CNN model with weights initialized by training once on our dataset. Initial loss is **0.3159**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "K73C2ytwTJYY",
    "outputId": "319211ed-df6c-4579-b1c5-c390a7b48806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 189s 31ms/step - loss: 0.3159\n"
     ]
    }
   ],
   "source": [
    "model = get_initialized_vgg(tr_gen, vggmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Encoder\n",
    "\n",
    "Now, we finally are at the point of finetuning the VGG16 net on our data. We continue to train the updated model for 5 epochs at a time. The evaluation is also handled by the model, for which we provide the test generator as well. Model is passed between each training session. We get the history which we use for looking at the training metrics.\n",
    "\n",
    "We save the model after every training session to keep a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "sK6GGYy3USiP",
    "outputId": "2a4b0654-3655-4043-b50a-d46fcd96e3ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 256s 43ms/step - loss: 0.1266 - binary_accuracy: 0.9682 - categorical_accuracy: 0.3500 - val_loss: 0.1094 - val_binary_accuracy: 0.9676 - val_categorical_accuracy: 0.3600\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 253s 42ms/step - loss: 0.0899 - binary_accuracy: 0.9729 - categorical_accuracy: 0.4207 - val_loss: 0.1072 - val_binary_accuracy: 0.9681 - val_categorical_accuracy: 0.3710\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 253s 42ms/step - loss: 0.0779 - binary_accuracy: 0.9760 - categorical_accuracy: 0.4345 - val_loss: 0.1068 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.3720\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 251s 42ms/step - loss: 0.0692 - binary_accuracy: 0.9785 - categorical_accuracy: 0.4472 - val_loss: 0.1069 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.3800\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 251s 42ms/step - loss: 0.0623 - binary_accuracy: 0.9806 - categorical_accuracy: 0.4538 - val_loss: 0.1072 - val_binary_accuracy: 0.9682 - val_categorical_accuracy: 0.3790\n"
     ]
    }
   ],
   "source": [
    "model,hist= train_vgg(tr_gen,te_gen,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after 5 iterations, loss has come down to **0.0623**. Training accuracy has increased for binary and categorical to **0.98 and 0.45**. For validation sets, they are **.96 and 0.38**. This is pretty good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e34ZKxVV1fa2"
   },
   "outputs": [],
   "source": [
    "model.save(\"ftv3004096_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "AiT-cwp561kD",
    "outputId": "36022b57-6cb8-49eb-b76c-7cca88bdf8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "3sgwP0g1-4e_",
    "outputId": "320f1b43-80fd-4afd-f5a2-a8ac86b8eb71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 251s 42ms/step - loss: 0.0564 - binary_accuracy: 0.9826 - categorical_accuracy: 0.4533 - val_loss: 0.1078 - val_binary_accuracy: 0.9678 - val_categorical_accuracy: 0.3710\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 250s 42ms/step - loss: 0.0512 - binary_accuracy: 0.9843 - categorical_accuracy: 0.4533 - val_loss: 0.1085 - val_binary_accuracy: 0.9678 - val_categorical_accuracy: 0.3650\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 250s 42ms/step - loss: 0.0466 - binary_accuracy: 0.9859 - categorical_accuracy: 0.4508 - val_loss: 0.1093 - val_binary_accuracy: 0.9678 - val_categorical_accuracy: 0.3660\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 250s 42ms/step - loss: 0.0425 - binary_accuracy: 0.9873 - categorical_accuracy: 0.4480 - val_loss: 0.1103 - val_binary_accuracy: 0.9678 - val_categorical_accuracy: 0.3680\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 250s 42ms/step - loss: 0.0388 - binary_accuracy: 0.9886 - categorical_accuracy: 0.4497 - val_loss: 0.1113 - val_binary_accuracy: 0.9678 - val_categorical_accuracy: 0.3690\n"
     ]
    }
   ],
   "source": [
    "model,hist= train_vgg(tr_gen,te_gen,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after 10 iterations, loss has come down to **0.0388**. Training accuracy has increased for binary and categorical to **0.986**. For validation sets, they are **.97**. categorical accuracy has reduced a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xj-9x2Ht-42E"
   },
   "outputs": [],
   "source": [
    "model.save(\"ftv3004096_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "d4gJNEQk8zh-",
    "outputId": "48610d0b-790d-4f4a-e60f-590b5579234a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 251s 42ms/step - loss: 0.0355 - binary_accuracy: 0.9897 - categorical_accuracy: 0.4493 - val_loss: 0.1123 - val_binary_accuracy: 0.9676 - val_categorical_accuracy: 0.3660\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 249s 42ms/step - loss: 0.0325 - binary_accuracy: 0.9909 - categorical_accuracy: 0.4500 - val_loss: 0.1137 - val_binary_accuracy: 0.9676 - val_categorical_accuracy: 0.3750\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 249s 42ms/step - loss: 0.0297 - binary_accuracy: 0.9919 - categorical_accuracy: 0.4485 - val_loss: 0.1151 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3800\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 249s 42ms/step - loss: 0.0272 - binary_accuracy: 0.9928 - categorical_accuracy: 0.4493 - val_loss: 0.1165 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3810\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 249s 41ms/step - loss: 0.0250 - binary_accuracy: 0.9936 - categorical_accuracy: 0.4483 - val_loss: 0.1181 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3840\n"
     ]
    }
   ],
   "source": [
    "model,hist= train_vgg(tr_gen,te_gen,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after 15 iterations, loss has come down to **0.025**. Training accuracy has increased for binary to **0.994**. Other accuracy metrics have remained more or less same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcmG6ZyDUEiA"
   },
   "outputs": [],
   "source": [
    "model.save(\"ftv3004096_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "sSNe5b7mKvd-",
    "outputId": "c1857ab4-8a17-41b1-fa83-a0728447e138",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 252s 42ms/step - loss: 0.0230 - binary_accuracy: 0.9943 - categorical_accuracy: 0.4515 - val_loss: 0.1193 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3810\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 250s 42ms/step - loss: 0.0211 - binary_accuracy: 0.9950 - categorical_accuracy: 0.4497 - val_loss: 0.1210 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3840\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 249s 42ms/step - loss: 0.0195 - binary_accuracy: 0.9957 - categorical_accuracy: 0.4508 - val_loss: 0.1225 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3860\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 249s 42ms/step - loss: 0.0179 - binary_accuracy: 0.9962 - categorical_accuracy: 0.4523 - val_loss: 0.1241 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3840\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 249s 42ms/step - loss: 0.0166 - binary_accuracy: 0.9967 - categorical_accuracy: 0.4513 - val_loss: 0.1257 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.3800\n"
     ]
    }
   ],
   "source": [
    "model,hist= train_vgg(tr_gen,te_gen,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmPzc6QzK13i"
   },
   "source": [
    "I stop after 3 training sessions to avoid overtraining as test accuracy is not improving.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have fine tuned the default VGG16 on the Flickr8k dataset which seems to improve its performance. The real test is performance improvement of the overall image captioning system which tells us if this exercise has benefitted the system."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "finetunevgg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

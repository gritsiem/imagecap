{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "merge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ILHNs1HU54RX",
        "outputId": "e42e57b9-8250-4689-b1cf-75ce94b93e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ruD_RZWS6wPE",
        "outputId": "16081b81-54d9-43fc-8469-64d24a2c23bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 24.5MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 4.7MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 6.8MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 4.4MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 5.3MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 6.3MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 7.2MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 8.0MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 8.9MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 7.1MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 7.1MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 9.6MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 9.5MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 16.7MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 16.9MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 16.8MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 16.4MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 16.5MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 16.6MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 42.6MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 21.4MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 21.3MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 21.3MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 21.2MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 21.2MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 20.2MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 21.1MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 21.1MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 21.0MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 21.6MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 45.8MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 46.6MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 47.8MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 44.0MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 43.6MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 48.3MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 48.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 30.7MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 30.1MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 30.0MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 29.2MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 29.4MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 29.8MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 30.5MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 30.7MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 30.6MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 30.5MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 49.3MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 50.0MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 47.4MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 49.4MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 24.5MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 24.0MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 24.5MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 23.7MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 23.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 23.5MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 23.4MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 23.5MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 24.0MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 24.1MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 46.1MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 47.9MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 48.8MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 42.2MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 42.1MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 42.8MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 42.4MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 42.7MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 42.1MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 42.2MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 43.6MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 44.3MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 44.4MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 55.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 53.6MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 53.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 53.8MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 52.9MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 54.6MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 53.5MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 53.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 51.4MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 46.7MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 46.7MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 48.1MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 47.2MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 47.9MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 48.5MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 47.8MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 48.0MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 46.8MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 48.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 52.4MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 52.0MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 52.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.4MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "98t5M_VGI_zU",
        "outputId": "db7879e0-c571-476b-a64d-aa277e0144d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/fine_tune_myself\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fine_tune_myself\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vZZyeEMg6Dal",
        "outputId": "96e6bccb-06be-4d65-c744-20ee278a220f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        " \n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        " \n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features\n",
        " \n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        " \n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        " \n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)\n",
        " \n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "  # decoder model\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(512, activation='relu')(decoder1)\n",
        "  decoder3 = Dense(256, activation='relu')(decoder2)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder3)\n",
        "\t# tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "  # summarize model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "\n",
        "def data_generator(descriptions, photos, tokenizer, max_length):\n",
        "  # loop for ever over images\n",
        "  while 1:\n",
        "    for key, desc_list in descriptions.items():\n",
        "      # retrieve the photo feature\n",
        "      photo = photos[key]\n",
        "      photo=photo.reshape(4096)\n",
        "      in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
        "      yield [[in_img, in_seq], out_word]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5fUlXBZ_6Yiu"
      },
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filename = '/content/drive/My Drive/image cap/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "test=load_set('/content/drive/My Drive/fine_tune_myself/Flickr_8k.testImages.txt')\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('/content/drive/My Drive/image cap/descriptions2.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('/content/drive/My Drive/fine_tune_myself/fvgg4096best.pkl', train)\n",
        "# prepare tokenizer\n",
        "test_desc=load_clean_descriptions('/content/drive/My Drive/image cap/descriptions2.txt', test)\n",
        "test_features = load_photo_features('/content/drive/My Drive/fine_tune_myself/fvgg4096best.pkl', test)\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_len = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_len)\n",
        " \n",
        "# define the model\n",
        "model = define_model(vocab_size, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NV6SuZuK6f5h",
        "outputId": "8f55c437-15fd-4a69-bd72-33c3282afe56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "  # create the data generator\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, max_len)\n",
        "  test_gen=data_generator(test_desc, test_features, tokenizer, max_len)\n",
        "  # fit for one epoch\n",
        "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1,validation_data=test_gen,validation_steps=1000)\n",
        "  # save model\n",
        "  model.save('ftv4096_256_merge' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 734s 122ms/step - loss: 4.3685 - acc: 0.2309 - val_loss: 3.9157 - val_acc: 0.2690\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 733s 122ms/step - loss: 3.6235 - acc: 0.2886 - val_loss: 3.7054 - val_acc: 0.2941\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 734s 122ms/step - loss: 3.3881 - acc: 0.3075 - val_loss: 3.6400 - val_acc: 0.3026\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 727s 121ms/step - loss: 3.2610 - acc: 0.3177 - val_loss: 3.6139 - val_acc: 0.3078\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 719s 120ms/step - loss: 3.1770 - acc: 0.3242 - val_loss: 3.6045 - val_acc: 0.3138\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 730s 122ms/step - loss: 3.1203 - acc: 0.3298 - val_loss: 3.6125 - val_acc: 0.3122\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 737s 123ms/step - loss: 3.0777 - acc: 0.3336 - val_loss: 3.6181 - val_acc: 0.3158\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 735s 122ms/step - loss: 3.0436 - acc: 0.3365 - val_loss: 3.6202 - val_acc: 0.3149\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 744s 124ms/step - loss: 3.0198 - acc: 0.3391 - val_loss: 3.6262 - val_acc: 0.3179\n",
            "Epoch 1/1\n",
            "4949/6000 [=======================>......] - ETA: 2:01 - loss: 3.0009 - acc: 0.3406Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}